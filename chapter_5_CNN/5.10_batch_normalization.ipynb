{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量标准化\n",
    "# 为了应对深度模型训练的挑战，在训练模型时，批量归一化利用小批量的\n",
    "# 上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的\n",
    "# 的中间输出的数值更加稳定。\n",
    "# 批量归一化和残差网络为训练和设计深度模型提供了两类重要的思路\n",
    "\n",
    "# 批量归一化层\n",
    "# 对全连接层和卷积层做批量归一化的方法稍有不同\n",
    "\n",
    "# 对全连接层的批量归一化\n",
    "# 仿射变换之后，激活函数之前，f（BN(x)）,X_hat = BN(x)引入了两个参数，拉伸参数gamma和偏移参数beta\n",
    "# 这两个参数和X_hat的形状相同,按照元素乘法和加法运算，这两个参数是可学习的，并且按照理论，\n",
    "# 如果学出来的gamma和beta满足最终的y与x相同，则表示学出的模型没有批量归一化的必要，符合逻辑\n",
    "\n",
    "# 对卷积层的批量归一化\n",
    "# 在卷积计算之后，应用激活函数之前，如果卷及计算输出多个通道，需要对这些通道的输出分别做批量归一化\n",
    "# 每个通道都有独立的拉伸和偏移参数，均为标量。\n",
    "\n",
    "# 预测时的批量归一化\n",
    "# 从零开始实现\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batch_norm(is_training,x,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得到的均值和方差\n",
    "        x_hat = (x - moving_mean)/torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(x.shape) in (2,4)\n",
    "        if len(x.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = x.mean(dim=0)\n",
    "            var = ((x-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差，这里我们需要保持\n",
    "            # x的形状以便后面做广播运算\n",
    "            mean = x.mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "            var = ((x - mean)**2).mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "        # 训练模式下使用当前的均值和方差做标准化\n",
    "        x_hat = (x-mean)/torch.sqrt(var+eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum*moving_mean + (1.0 - momentum)*mean\n",
    "        moving_var = momentum*moving_var + (1.0 - momentum)*var\n",
    "    Y = gamma * x_hat + beta # 拉伸和偏移\n",
    "    return Y,moving_mean,moving_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来自定义一个BatchNorm层\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features,num_dims):\n",
    "        super(BatchNorm,self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1,num_features)\n",
    "        else:\n",
    "            shape = (1,num_features,1,1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # 如果x不在内存上，将moving_mean和moving_var复制到x所在的显存上\n",
    "        if self.moving_mean.device != x.device:\n",
    "            self.moving_mean = self.moving_mean.to(x.device)\n",
    "            self.moving_var = self.moving_var.to(x.device)\n",
    "        Y,self.moving_mean,self.moving_var = batch_norm(self.traing,x,self.gamma,self.beta,self.moving_mean,\n",
    "                                                       self.moving_var,eps = 1e-5,momentum=0.9)\n",
    "        return Y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用批量归一化层的LeNet\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1,6,5),\n",
    "    BatchNorm(6,num_dims = 4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2), # kernel_size,stride\n",
    "    \n",
    "    nn.Conv2d(6,16,5),\n",
    "    BatchNorm(16,num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    \n",
    "    d2l.FalttenLayer(),\n",
    "    nn.Linear(16*4*4,120),\n",
    "    BatchNorm(120,num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    \n",
    "    nn.Linear(120,84),\n",
    "    BatchNorm(84,num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    \n",
    "    nn.Linear(84,10)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简洁实现   直接使用nn库中的BatchNorm1d和BatchNorm2d来实现（1d是全连接层，2d是卷积层的BN）\n",
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
